<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Mohammad Arafat Hussain, PhD</title> <meta name="author" content="Mohammad Arafat Hussain, PhD"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://marafathussain.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mohammad Arafat </span>Hussain, PhD</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ASPNR</abbr></div> <div id="Kesri2024therole" class="col-sm-8"> <div class="title">The Role of Expert MRI Scores in Predicting Adverse 18-22-month Outcomes for Hypoxic Ischemic Encephalopathy in Neonatal Research Network Trials</div> <div class="author"> Ankush Kesri, Rina Bao, Chuan-Heng Hsiao, Rutvi Vyas, <em>Mohammad Arafat Hussain</em>, Scott McDonald, Jeanette Auman, Seetha Shankaran, Abbott Laptook, Michael Cotten, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ellen Grant, Yangming Ou' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 7th Annual Meeting of American Society of Pediatric Neuroradiology</em> 2025 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ASPNR_abstract_2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Kesri2024therole</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Role of Expert MRI Scores in Predicting Adverse 18-22-month Outcomes for Hypoxic Ischemic Encephalopathy in Neonatal Research Network Trials}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kesri, Ankush and Bao, Rina and Hsiao, Chuan-Heng and Vyas, Rutvi and Hussain, Mohammad Arafat and McDonald, Scott and Auman, Jeanette and Shankaran, Seetha and Laptook, Abbott and Cotten, Michael and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{7th Annual Meeting of American Society of Pediatric Neuroradiology}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{ASPNR}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PRIME</abbr></div> <div id="hussain2024rct" class="col-sm-8"> <div class="title">RCT: Relational Connectivity Transformer for Enhanced Prediction of Absolute and Residual Intelligence</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <a href="https://www.childrenshospital.org/research/researchers/yangming-ou" target="_blank" rel="noopener noreferrer">Yangming Ou</a> </div> <div class="periodical"> <em>In 7th International Workshop of Predictive Intelligence in Medicine (PRIME)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/prime2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/marafathussain/RelationalConnectivityTransformer" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This paper introduces the Relational Connectivity Transformer (RCT), a novel Graph-Transformer model designed for predicting absolute and residual full-scale intelligence quotient (FSIQ), performance IQ (PIQ), and verbal IQ (VIQ) scores from resting-state functional magnetic resonance imaging (rs-fMRI) data. Early prediction of neurocognitive impairments via IQ scores may allow for timely intervention. To this end, our RCT model leverages a relation-learning strategy from paired sample data via a novel graph-based transformer framework. Through a comprehensive comparison with state-of-the-art approaches in a 5-fold cross-validation setup, our model demonstrated superior performance. Statistical analysis confirmed the significant improvement (p &lt; 0.05) in FSIQ prediction, strengthening the efficacy of the proposed method. This work marks the first application of a Graph-Transformer in predicting IQ scores using rs-fMRI, introducing a novel learning strategy and contributing to the ongoing efforts to enhance the accuracy and reliability of human intelligence predictions based on functional brain connectivity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2024rct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RCT: Relational Connectivity Transformer for Enhanced Prediction of Absolute and Residual Intelligence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{7th International Workshop of Predictive Intelligence in Medicine (PRIME)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MEP</abbr></div> <div id="marhamati2024patient" class="col-sm-8"> <div class="title">Patient’s Airway Monitoring during Cardiopulmonary Resuscitation using Deep Networks</div> <div class="author"> Mahmoud Marhamati, Behnam Dorri, Shima Imannezhad, <em>Mohammad Arafat Hussain</em>, Ali Asghar Neshat, Abulfazl Kakmishi, and Mohammad Momeny</div> <div class="periodical"> <em>Medical Engineering &amp; Physics</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1350453324000808" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2024a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Cardiopulmonary resuscitation (CPR) is a crucial life-saving technique commonly administered to individuals experiencing cardiac arrest. Among the important aspects of CPR is ensuring the correct airway position of the patient, which is typically monitored by human tutors or supervisors. This study aims to utilize deep transfer learning for the detection of the patient’s correct and incorrect airway position during cardiopulmonary resuscitation. To address the challenge of identifying the airway position, we curated a dataset consisting of 198 recorded video sequences, each lasting 6-8 seconds, showcasing both correct and incorrect airway positions during mouth-to-mouth breathing and breathing with an Ambu Bag. We employed six cutting-edge deep networks, namely DarkNet19, EfficientNetB0, GoogleNet, MobileNet-v2, ResNet50, and NasnetMobile. These networks were initially pre-trained on computer vision data and subsequently fine-tuned using the CPR dataset. The validation of the fine-tuned networks in detecting the patient’s correct airway position during mouth-to-mouth breathing achieved impressive results, with the best sensitivity (98.8%), specificity (100%), and F-measure (97.2%). Similarly, the detection of the patient’s correct airway position during breathing with an Ambu Bag exhibited excellent performance, with the best sensitivity (100%), specificity (99.8%), and F-measure (99.7%).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">marhamati2024patient</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.medengphy.2024.104179}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Patient's Airway Monitoring during Cardiopulmonary Resuscitation using Deep Networks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marhamati, Mahmoud and Dorri, Behnam and Imannezhad, Shima and Hussain, Mohammad Arafat and Neshat, Ali Asghar and Kakmishi, Abulfazl and Momeny, Mohammad}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Medical Engineering \&amp; Physics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104179}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">bioRxiv</abbr></div> <div id="hussain2023enhancing" class="col-sm-8"> <div class="title">Enhancing Neurocognitive Outcome Prediction in Congenital Heart Disease Patients: The Role of Brain Age Biomarkers and Beyond</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <a href="https://www.childrenshospital.org/research/researchers/yangming-ou" target="_blank" rel="noopener noreferrer">Yangming Ou</a> </div> <div class="periodical"> <em>bioRxiv</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.biorxiv.org/content/10.1101/2023.09.01.555976v1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/CHD_Neuro_2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper aimed to investigate the predictive power of combining demographic, socioeconomic, and genetic factors with a brain MRI-based quantified measure of accelerated brain aging (referred to as deltaAGE) for neurocognitive outcomes in adolescents and young adults with Congenital Heart Disease (CHD). Our hypothesis posited that including the brain age biomarker (deltaAGE) would enhance neurocognitive outcome predictions compared to models excluding it. We conducted comprehensive analyses, including leave-one-subject-out and leave-one-group-out cross-validation techniques. Our results demonstrated that the inclusion of deltaAGE consistently improved prediction performance when considering the Pearson correlation coefficient, a preferable metric for this study. Notably, the deltaAGE-augmented models consistently outperformed those without deltaAGE across all cross-validation setups, and these correlations were statistically significant (p-value &lt; 0.05). Therefore, our hypothesis that incorporating the brain-age biomarker alongside demographic, socioeconomic, and genetic factors enhances neurocognitive outcome predictions in adolescents and young adults with CHD is supported by the findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2023enhancing</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1101/2023.09.01.555976}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2023-02}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Enhancing Neurocognitive Outcome Prediction in Congenital Heart Disease Patients: The Role of Brain Age Biomarkers and Beyond}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Displays</abbr></div> <div id="MARHAMATI2023102371" class="col-sm-8"> <div class="title">LAIU-Net: A learning-to-augment incorporated robust U-Net for depressed humans’ tongue segmentation</div> <div class="author"> Mahmoud Marhamati, Ali Asghar Latifi Zadeh, Masoud Mojdehi Fard, <em>Mohammad Arafat Hussain</em>, Khalegh Jafarnezhad, Ahad Jafarnezhad, Mehdi Bakhtoor, and Mohammad Momeny</div> <div class="periodical"> <em>Displays</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0141938223000045" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/displays2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mohamadmomeny/Learning-to-augment-strategy" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Computer-aided tongue diagnosis system requires segmentation of the tongue body. The frequent movement of the tongue due to its natural flexibility often causes shape variability in photographs across subjects, which makes segmenting the tongue challenging from non-tongue elements, such as the lips, teeth, and other objects in the background of the tongue. The flexibility of the tongue causes a further challenge in maintaining a similar shape and style when taking photos of many healthy subjects and patients. To address these challenges, we have built a tongue dataset, where the tongue of each subject has been scanned thrice with an interval of less than a second. We have collected 333 tongue images from 111 depressed humans, who have been diagnosed with depression by a psychiatrist. In addition, in this paper, we propose a learning-to-augment incorporated U-Net (LAIU-Net) for the segmentation of the depressed human tongue in photographic images. The best policies for data augmentation were automatically chosen with the proposed LAIU-Net. For this purpose, we corrupted photographic tongue images with the Gaussian, speckle, and Poisson noise. The proposed approach addresses the overfitting problem as well as increases the generalizability of a deep network. We have compared the performance of the proposed LAIU-Net with that of other state-of-the-art U-Net configurations. Our LAIU-Net approach achieved a mean boundary F1 score of 93.1%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">MARHAMATI2023102371</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{LAIU-Net: A learning-to-augment incorporated robust U-Net for depressed humans’ tongue segmentation}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Displays}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{102371}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0141-9382}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.displa.2023.102371}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marhamati, Mahmoud and Zadeh, Ali Asghar Latifi and Fard, Masoud Mojdehi and Hussain, Mohammad Arafat and Jafarnezhad, Khalegh and Jafarnezhad, Ahad and Bakhtoor, Mehdi and Momeny, Mohammad}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Tongue segmentation, learning-to-augment strategy, data augmentation, deep learning, U-Net}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprints</abbr></div> <div id="morshed2023ultrasound" class="col-sm-8"> <div class="title">Ultrasound-Based AI for COVID-19 Detection: A Comprehensive Review of Public and Private Lung Ultrasound Datasets and Studies</div> <div class="author"> Abrar Morshed, Abdulla Al Shihab, Md Abrar Jahin, Md Jaber Al Nahian, Md Murad Hossain Sarker, Md Sharjis Ibne Wadud, Mohammad Istiaq Uddin, Muntequa Imtiaz Siraji, Nafisa Anjum, Sumiya Rajjab Shristy, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? ' others' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>Preprints</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.preprints.org/manuscript/202303.0296/v3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.preprints.org/manuscript/202303.0296/v3/download" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The COVID-19 pandemic has affected millions of people globally, with respiratory organs being strongly affected in individuals with comorbidities. Medical imaging-based diagnosis and prognosis have become increasingly popular in clinical settings to detect COVID-19 lung infections. Among various medical imaging modalities, ultrasound stands out as low-cost, mobile, and radiation-safe imaging technology. In this comprehensive review, we focus on ultrasound-based AI studies for COVID-19 detection that use public or private lung ultrasound datasets. We surveyed articles that used publicly available lung ultrasound datasets for COVID-19 and reviewed publicly available datasets and organize ultrasound-based AI studies per dataset. We analyzed and tabulated studies in several dimensions, such as data preprocessing, AI models, cross-validation, and evaluation criteria. In total, we reviewed 42 articles, where 28 articles used public datasets, and the rest used private data. Our findings suggest that ultrasound-based AI studies for the detection of COVID-19 have great potential for clinical use, especially for children and pregnant women. Our review also provides a useful summary for future researchers and clinicians who may be interested in the field.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">morshed2023ultrasound</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.20944/preprints202303.0296.v3}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprints}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Ultrasound-Based AI for COVID-19 Detection: A Comprehensive Review of Public and Private Lung Ultrasound Datasets and Studies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Morshed, Abrar and Al Shihab, Abdulla and Jahin, Md Abrar and Al Nahian, Md Jaber and Sarker, Md Murad Hossain and Wadud, Md Sharjis Ibne and Uddin, Mohammad Istiaq and Siraji, Muntequa Imtiaz and Anjum, Nafisa and Shristy, Sumiya Rajjab and others}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Preprints}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">bioRxiv</abbr></div> <div id="hussain2023can" class="col-sm-8"> <div class="title">Can deep learning predict human intelligence from structural brain MRI?</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Danielle LaMay, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <a href="https://www.childrenshospital.org/research/researchers/yangming-ou" target="_blank" rel="noopener noreferrer">Yangming Ou</a> </div> <div class="periodical"> <em>bioRxiv</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.biorxiv.org/content/10.1101/2023.02.24.529924v1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.biorxiv.org/content/10.1101/2023.02.24.529924v1.full.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/i3-research/MRI-infer-neurocognition" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Can brain structure predict human intelligence? T1-weighted structural brain magnetic resonance images (sMRI) have been correlated with intelligence. Nevertheless, population-level association does not fully account for individual variability in intelligence. To address this, individual prediction studies emerge recently. However, they are mostly on predicting fluid intelligence (the ability to solve new problems). Studies are lacking to predict crystallized intelligence (the ability to accumulate knowledge) or general intelligence (fluid and crystallized intelligence combined). This study tests whether deep learning of sMRI can predict an individual subject’s verbal, comprehensive, and full-scale intelligence quotients (VIQ, PIQ, FSIQ), which reflect both fluid and crystallized intelligence. We performed a comprehensive set of 432 experiments, using different input images, six deep learning models, and two outcome settings, on 850 autistic and healthy subjects 6-64 years of age. Results show promise with statistical significance, and also open up questions inviting further future studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2023can</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1101/2023.02.24.529924}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Can deep learning predict human intelligence from structural brain MRI?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and LaMay, Danielle and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2023--02}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">bioRxiv</abbr></div> <div id="hussain2023influence" class="col-sm-8"> <div class="title">Influence of Demographic, Socio-economic, and Brain Structural Factors on Adolescent Neurocognition: A Correlation Analysis in the ABCD Initiative</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Grace Li, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <a href="https://www.childrenshospital.org/research/researchers/yangming-ou" target="_blank" rel="noopener noreferrer">Yangming Ou</a> </div> <div class="periodical"> <em>bioRxiv</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.biorxiv.org/content/10.1101/2023.02.24.529930v1.abstract" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.biorxiv.org/content/biorxiv/early/2023/02/27/2023.02.24.529930.full.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>The Adolescent Brain Cognitive Development (ABCD) initiative is a longitudinal study aimed at characterizing brain development from childhood through adolescence and identifying key biological and environmental factors that influence this development. The study measures neurocognitive abilities across a multidimensional array of functions, with a focus on the critical period of adolescence during which physical and socio-emotional changes occur and the structure of the cortical and white matter changes. In this study, we perform a correlation analysis to examine the linear relation of adolescent neurocognition functions with the demographic, socio-economic, and magnetic resonance imaging-based brain structural factors. The overall goal is to obtain a comprehensive understanding of how natural and nurtural factors influence adolescent neurocognition. Our results on &gt; 10,000 adolescents show many positive and negative statistical significance interrelations of different neurocognitive functions with the demographic, socioeconomic, and brain structural factors, and also open up questions inviting further future studies.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2023influence</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1101/2023.02.24.529930}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Influence of Demographic, Socio-economic, and Brain Structural Factors on Adolescent Neurocognition: A Correlation Analysis in the ABCD Initiative}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Li, Grace and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{bioRxiv}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2023--02}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Cold Spring Harbor Laboratory}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprints</abbr></div> <div id="hussain2023inferring" class="col-sm-8"> <div class="title">Inferring Neurocognition and Intelligence using Brain MRI</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <a href="https://www.childrenshospital.org/research/researchers/yangming-ou" target="_blank" rel="noopener noreferrer">Yangming Ou</a> </div> <div class="periodical"> <em>Preprints</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.preprints.org/manuscript/202302.0452/v1" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://www.preprints.org/manuscript/202302.0452/v1/download" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Brain magnetic resonance imaging (MRI) offers a unique lens to study neuroanatomic support of human neurocognition and intelligence. A core mystery is the MRI explanation of individual differences in neurocognition and intelligence. The past four decades have seen great advancement in studying this century-long mystery, but the sample size and population-level studies limit the explanation at the individual level. The recent rise of big data and artificial intelligence offers novel opportunities. Yet, data sources, harmonization, study design, and interpretation need to be carefully considered. This review aims to summarize past work, discuss rising opportunities and challenges, and facilitate further investigations on machine intelligence inferring human intelligence.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2023inferring</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.20944/preprints202302.0452.v1}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Preprints}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Inferring Neurocognition and Intelligence using Brain MRI}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Preprints}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CMIG</abbr></div> <div id="HUSSAIN2022102127" class="col-sm-8"> <div class="title">Active deep learning from a noisy teacher for semi-supervised 3D image segmentation: Application to COVID-19 pneumonia infection in CT</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Zahra Mirikharaji, Mohammad Momeny, Mahmoud Marhamati, Ali Asghar Neshat, <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Garbi</a>, and <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a> </div> <div class="periodical"> <em>Computerized Medical Imaging and Graphics</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0895611122000970" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/cmig2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/marafathussain/ALNT" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Supervised deep learning has become a standard approach to solving medical image segmentation tasks. However, serious difficulties in attaining pixel-level annotations for sufficiently large volumetric datasets in real-life applications have highlighted the critical need for alternative approaches, such as semi-supervised learning, where model training can leverage small expert-annotated datasets to enable learning from much larger datasets without laborious annotation. Most of the semi-supervised approaches combine expert annotations and machine-generated annotations with equal weights within deep model training, despite the latter annotations being relatively unreliable and likely to affect model optimization negatively. To overcome this, we propose an active learning approach that uses an example re-weighting strategy, where machine-annotated samples are weighted (i) based on the similarity of their gradient directions of descent to those of expert-annotated data, and (ii) based on the gradient magnitude of the last layer of the deep model. Specifically, we present an active learning strategy with a query function that enables the selection of reliable and more informative samples from machine-annotated batch data generated by a noisy teacher. When validated on clinical COVID-19 CT benchmark data, our method improved the performance of pneumonia infection segmentation compared to the state of the art.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">HUSSAIN2022102127</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Active deep learning from a noisy teacher for semi-supervised 3D image segmentation: Application to COVID-19 pneumonia infection in CT}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computerized Medical Imaging and Graphics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{102127}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0895-6111}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.compmedimag.2022.102127}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Mirikharaji, Zahra and Momeny, Mohammad and Marhamati, Mahmoud and Neshat, Ali Asghar and Garbi, Rafeef and Hamarneh, Ghassan}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Deep learning, Semi-supervised learning, Active learning, Segmentation, Noisy teacher, COVID-19, Pneumonia}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">JCS</abbr></div> <div id="AKBARIMAJD2022101763" class="col-sm-8"> <div class="title">Learning-to-augment incorporated noise-robust deep CNN for detection of COVID-19 in noisy X-ray images</div> <div class="author"> Adel Akbarimajd, Nicolas Hoertel, <em>Mohammad Arafat Hussain</em>, Ali Asghar Neshat, Mahmoud Marhamati, Mahdi Bakhtoor, and Mohammad Momeny</div> <div class="periodical"> <em>Journal of Computational Science</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S1877750322001466" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/jsc2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mohamadmomeny/Learning-to-augment-strategy" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Deep convolutional neural networks (CNNs) are used for the detection of COVID-19 in X-ray images. The detection performance of deep CNNs may be reduced by noisy X-ray images. To improve the robustness of a deep CNN against impulse noise, we propose a novel CNN approach using adaptive convolution, with the aim to ameliorate COVID-19 detection in noisy X-ray images without requiring any preprocessing for noise removal. This approach includes an impulse noise-map layer, an adaptive resizing layer, and an adaptive convolution layer to the conventional CNN framework. We also used a learning-to-augment strategy using noisy X-ray images to improve the generalization of a deep CNN. We have collected a dataset of 2093 chest X-ray images including COVID-19 (452 images), non-COVID pneumonia (621 images), and healthy ones (1020 images). The architecture of pre-trained networks such as SqueezeNet, GoogleNet, MobileNetv2, ResNet18, ResNet50, ShuffleNet, and EfficientNetb0 has been modified to increase their robustness to impulse noise. Validation on the noisy X-ray images using the proposed noise-robust layers and learning-to-augment strategy-incorporated ResNet50 showed 2% better classification accuracy compared with state-of-the-art method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">AKBARIMAJD2022101763</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning-to-augment incorporated noise-robust deep CNN for detection of COVID-19 in noisy X-ray images}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Computational Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{63}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{101763}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1877-7503}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.jocs.2022.101763}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Akbarimajd, Adel and Hoertel, Nicolas and Hussain, Mohammad Arafat and Neshat, Ali Asghar and Marhamati, Mahmoud and Bakhtoor, Mahdi and Momeny, Mohammad}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Noise, Adaptive resize, Adaptive convolution, Data augmentation, COVID-19 classification}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ASNR</abbr></div> <div id="lankalapalli2022accelerated" class="col-sm-8"> <div class="title">Accelerated Brain Aging in Congenital Heart Disease and Relation to Neurodevelopmental Outcome</div> <div class="author"> Ruhika Lankalapalli, Sheng He, James Ko, <em>Mohammad Arafat Hussain</em>, Kiho Im, Ai Wern Chung, Patric Johnston, Jane Newburger, Sarah Morton, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Yangming Ou' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 60th Annual Meeting of the American Society of Neuroradiology</em> 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/ruhika_asnr_2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lankalapalli2022accelerated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Accelerated Brain Aging in Congenital Heart Disease and Relation to Neurodevelopmental Outcome}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lankalapalli, Ruhika and He, Sheng and Ko, James and Hussain, Mohammad Arafat and Im, Kiho and Chung, Ai Wern and Johnston, Patric and Newburger, Jane and Morton, Sarah and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{60th Annual Meeting of the American Society of Neuroradiology}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{ASNR}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Nature Conf.</abbr></div> <div id="hussain2022machine" class="col-sm-8"> <div class="title">Machine Intelligence to Predict Human Intelligence</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Danielle LaMay, <a href="https://www.childrenshospital.org/directory/patricia-ellen-grant" target="_blank" rel="noopener noreferrer">Ellen Grant</a>, and <a href="https://www.childrenshospital.org/research/researchers/yangming-ou" target="_blank" rel="noopener noreferrer">Yangming Ou</a> </div> <div class="periodical"> <em>In AI, neuroscience and hardware: From neural to artificial systems and back again</em> 2022 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/mah_nat_2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/Nature_AI_Poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2022machine</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Machine Intelligence to Predict Human Intelligence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and LaMay, Danielle and Grant, Ellen and Ou, Yangming}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AI, neuroscience and hardware: From neural to artificial systems and back again}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Nature Conferences}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CIBM</abbr></div> <div id="momeny2021learning" class="col-sm-8"> <div class="title">Learning-to-Augment Strategy using Noisy and Denoised Data: Improving Generalizability of Deep CNN for the Detection of COVID-19 in X-ray Images</div> <div class="author"> Mohammad Momeny, Ali Asghar Neshat, <em>Mohammad Arafat Hussain</em>, Solmaz Kia, Mahmoud Marhamati, Ahmad Jahanbakhshi, and <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a> </div> <div class="periodical"> <em>Computers in Biology and Medicine</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010482521004984" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/cibm2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/mohamadmomeny/Learning-to-augment-strategy" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Chest X-ray images are used in deep convolutional neural networks for the detection of COVID-19, the greatest human challenge of the 21st century. Robustness to noise and improvement of generalization are the major challenges in designing these networks. In this paper, we introduce a strategy for data augmentation using the determination of the type and value of noise density to improve the robustness and generalization of deep CNNs for COVID-19 detection. Firstly, we present a learning-to-augment approach that generates new noisy variants of the original image data with optimized noise density. We apply a Bayesian optimization technique to control and choose the optimal noise type and its parameters. Secondly, we propose a novel data augmentation strategy, based on denoised X-ray images, that uses the distance between denoised and original pixels to generate new data. We develop an autoencoder model to create new data using denoised images corrupted by the Gaussian and impulse noise. A database of chest X-ray images, containing COVID-19 positive images, normal images, and other non-COVID pneumonia images, is used to fine-tune the pre-trained networks (AlexNet, ShuffleNet, ResNet18, and GoogleNet). The proposed method has the better performance in comparison with the data augmentation approaches in terms of sensitivity (by 0.808), specificity (by 0.915), and F-Measure (by 0.737).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">momeny2021learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning-to-Augment Strategy using Noisy and Denoised Data: Improving Generalizability of Deep CNN for the Detection of COVID-19 in X-ray Images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Momeny, Mohammad and Neshat, Ali Asghar and Hussain, Mohammad Arafat and Kia, Solmaz and Marhamati, Mahmoud and Jahanbakhshi, Ahmad and Hamarneh, Ghassan}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computers in Biology and Medicine}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CMIG</abbr></div> <div id="hussain2021learnable" class="col-sm-8"> <div class="title">Learnable Image Histograms-based Deep Radiomics for Renal Cell Carcinoma Grading and Staging</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Garbi</a> </div> <div class="periodical"> <em>Computerized Medical Imaging and Graphics</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0895611121000732" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/cmig_arafat_2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/marafathussain/ImHistNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Fuhrman cancer grading and tumor-node-metastasis (TNM) cancer staging systems are typically used by clinicians in the treatment planning of renal cell carcinoma (RCC), a common cancer in men and women worldwide. Pathologists typically use percutaneous renal biopsy for RCC grading, while staging is performed by volumetric medical image analysis before renal surgery. Recent studies suggest that clinicians can effectively perform these classification tasks non-invasively by analysing image texture features of RCC from computed tomography (CT) data. However, image feature identification for RCC grading and staging often relies on laborious manual processes, which is error prone and time-intensive. To address this challenge, this paper proposes a learnable image histogram in the deep neural network framework that can learn task-specific image histograms with variable bin centers and widths. The proposed approach enables learning statistical context features from raw medical data, which cannot be performed by a conventional convolutional neural network (CNN). The linear basis function of our learnable image histogram is piece-wise differentiable, enabling back-propagating errors to update the variable bin centers and widths during training. This novel approach can segregate the CT textures of an RCC in different intensity spectra, which enables effcient Fuhrman low (I/II) and high (III/IV) grading as well as RCC low (I/II) and high (III/IV) staging. The proposed method is validated on a clinical CT dataset of 159 patients from The Cancer Imaging Archive (TCIA) database, and it demonstrates 80% and 83% accuracy in RCC grading and staging, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2021learnable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learnable Image Histograms-based Deep Radiomics for Renal Cell Carcinoma Grading and Staging}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computerized Medical Imaging and Graphics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TMI</abbr></div> <div id="hussain2021cascaded" class="col-sm-8"> <div class="title">Cascaded Localization Regression Neural Nets for Kidney Localization and Segmentation-free Volume Estimation</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Garbi</a> </div> <div class="periodical"> <em>IEEE Transactions on Medical Imaging</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9358223?casa_token=rxZNi4GaP-YAAAAA:vlaAvOf6J1pKBT9goM4k0cCgPyJQ9NgOg_SSzt4iAFwHINOSelv-LsPXU44-XYmkME_wsI8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/tmi2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Kidney volume is an essential biomarker for a number of kidney disease diagnoses, for example, chronic kidney disease. Existing total kidney volume estimation methods often rely on an intermediate kidney segmentation step. On the other hand, automatic kidney localization in volumetric medical images is a critical step that often precedes subsequent data processing and analysis. Most current approaches perform kidney localization via an intermediate classification or regression step. This paper proposes an integrated deep learning approach for (i) kidney localization in computed tomography scans and (ii) segmentation-free renal volume estimation. Our localization method uses a selection-convolutional neural network that approximates the kidney inferior-superior span along the axial direction. Cross-sectional (2D) slices from the estimated span are subsequently used in a combined sagittal-axial Mask-RCNN that detects the organ bounding boxes on the axial and sagittal slices, the combination of which produces a final 3D organ bounding box. Furthermore, we use a fully convolutional network to estimate the kidney volume that skips the segmentation procedure. We also present a mathematical expression to approximate the ‘volume error’ metric from the ‘Sørensen–Dice coefficient.’ We accessed 100 patients’ CT scans from the Vancouver General Hospital records and obtained 210 patients’ CT scans from the 2019 Kidney Tumor Segmentation Challenge database to validate our method. Our method produces a kidney boundary wall localization error of  2.4mm and a mean volume estimation error of  5%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2021cascaded</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cascaded Localization Regression Neural Nets for Kidney Localization and Segmentation-free Volume Estimation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Medical Imaging}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PhD Thesis</abbr></div> <div id="hussain2020volumetric" class="col-sm-8"> <div class="title">Volumetric image-based supervised learning approaches for kidney cancer detection and analysis</div> <div class="author"> <em>Mohammad Arafat Hussain</em> </div> <div class="periodical"> 2020 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0389786" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://open.library.ubc.ca/media/download/pdf/24/1.0389786/4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@phdthesis</span><span class="p">{</span><span class="nl">hussain2020volumetric</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Volumetric image-based supervised learning approaches for kidney cancer detection and analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">school</span> <span class="p">=</span> <span class="s">{University of British Columbia}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI</abbr></div> <div id="hussain2019imhistnet" class="col-sm-8"> <div class="title">ImHistNet: Learnable image histogram based DNN with application to noninvasive determination of carcinoma grades in CT scans</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Garbi</a> </div> <div class="periodical"> <em>In International Conference on Medical Image Computing and Computer-Assisted Intervention</em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-32226-7_15" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mahmiccai19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/marafathussain/ImHistNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Renal cell carcinoma (RCC) is the seventh most common cancer worldwide, accounting for an estimated 140,000 global deaths annually. Clear cell RCC (ccRCC) is the major subtype of RCC and its biological aggressiveness affects prognosis and treatment planning. An important ccRCC prognostic predictor is its ‘grade’ for which the 4-tiered Fuhrman grading system is used. Although the Fuhrman grade can be identified by percutaneous renal biopsy, recent studies suggested that such grades may be non-invasively identified by studying image texture features of the ccRCC from computed tomography (CT) data. Such image feature based identification currently mostly relies on laborious manual processes based on visual inspection of 2D image slices that are time-consuming and subjective. In this paper, we propose a learnable image histogram based deep neural network approach that can perform the Fuhrman low (I/II) and high (III/IV) grade classification for ccRCC in CT scans. Validated on a clinical CT dataset of 159 patients from the TCIA database, our method classified ccRCC low and high grades with 80% accuracy and 85% AUC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2019imhistnet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ImHistNet: Learnable image histogram based DNN with application to noninvasive determination of carcinoma grades in CT scans}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Medical Image Computing and Computer-Assisted Intervention}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{130--138}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI-MLMI</abbr></div> <div id="hussain2019renal" class="col-sm-8"> <div class="title">Renal Cell Carcinoma Staging with Learnable Image Histogram-Based Deep Neural Network</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Garbi</a> </div> <div class="periodical"> <em>In International Workshop on Machine Learning in Medical Imaging</em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-32692-0_61" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mahmlmi19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/marafathussain/ImHistNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Renal cell carcinoma (RCC) is the seventh most common cancer worldwide, accounting for an estimated 140,000 global deaths annually. An important RCC prognostic predictor is its ‘stage’ for which the tumor-node-metastasis (TNM) staging system is used. Although TNM staging is performed by radiologists via pre-surgery volumetric medical image analysis, a recent study suggested that such staging may be performed by studying the image features of the RCC from computed tomography (CT) data. Currently TNM staging mostly relies on laborious manual processes based on visual inspection of 2D CT image slices that are time-consuming and subjective; a recent study reported about ∼ 25% misclassification in their patient pools. Recently, we proposed a learnable image histogram based deep neural network approach (ImHistNet) for RCC grading, which is capable of learning textural features directly from the CT images. In this paper, using a similar architecture, we perform the stage low (I/II) and high (III/IV) classification for RCC in CT scans. Validated on a clinical CT dataset of 159 patients from the TCIA database, our method classified RCC low and high stages with about 83% accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2019renal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Renal Cell Carcinoma Staging with Learnable Image Histogram-Based Deep Neural Network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Workshop on Machine Learning in Medical Imaging}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{533--540}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2018</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI</abbr></div> <div id="hussain2018noninvasive" class="col-sm-8"> <div class="title">Noninvasive determination of gene mutations in clear cell renal cell carcinoma using multiple instance decisions aggregated CNN</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Garbi</a> </div> <div class="periodical"> <em>In International Conference on Medical Image Computing and Computer-Assisted Intervention</em> 2018 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-030-00934-2_73" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/Arafat_MICCAI18.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Kidney clear cell renal cell carcinoma (ccRCC) is the major sub-type of RCC, constituting one the most common cancers worldwide accounting for a steadily increasing mortality rate with 350,000 new cases recorded in 2012. Understanding the underlying genetic mutations in ccRCC provides crucial information enabling malignancy staging and patient survival estimation thus plays a vital role in accurate ccRCC diagnosis, prognosis, treatment planning, and response assessment. Although the underlying gene mutations can be identified by whole genome sequencing of the ccRCC following invasive nephrectomy or kidney biopsy procedures, recent studies have suggested that such mutations may be noninvasively identified by studying image features of the ccRCC from Computed Tomography (CT) data. Such image feature identification currently relies on laborious manual processes based on visual inspection of 2D image slices that are time-consuming and subjective. In this paper, we propose a convolutional neural network approach for automatic detection of underlying ccRCC gene mutations from 3D CT volumes. We aggregate the mutation-presence/absence decisions for all the ccRCC slices in a kidney into a robust singular decision that determines whether the interrogated kidney bears a specific mutation or not. When validated on clinical CT datasets of 267 patients from the TCIA database, our method detected gene mutations with 94% accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2018noninvasive</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Noninvasive determination of gene mutations in clear cell renal cell carcinoma using multiple instance decisions aggregated CNN}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hamarneh, Ghassan and Garbi, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Medical Image Computing and Computer-Assisted Intervention}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{657--665}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI</abbr></div> <div id="hussain2017segmentation" class="col-sm-8"> <div class="title">Segmentation-free kidney localization and volume estimation using aggregated orthogonal decision CNNs</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Alborz Amir-Khalili, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Abugharbieh</a> </div> <div class="periodical"> <em>In International Conference on Medical Image Computing and Computer-Assisted Intervention</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-66179-7_70" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/Arafat_MICCAI17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Kidney volume is an important bio-marker in the clinical diagnosis of various renal diseases. For example, it plays an essential role in follow-up evaluation of kidney transplants. Most existing methods for volume estimation rely on kidney segmentation as a prerequisite step, which has various limitations such as initialization-sensitivity and computationally-expensive optimization. In this paper, we propose a hybrid localization-volume estimation deep learning approach capable of (i) localizing kidneys in abdominal CT images, and (ii) estimating renal volume without requiring segmentation. Our approach involves multiple levels of self-learning of image representation using convolutional neural layers, which we show better capture the rich and complex variability in kidney data, demonstrably outperforming hand-crafted feature representations. We validate our method on clinical data of 100 patients with a total of 200 kidney samples (left and right). Our results demonstrate a 55% increase in kidney boundary localization accuracy, and a 30% increase in volume estimation accuracy compared to recent state-of-the-art methods deploying regression-forest-based learning for the same tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2017segmentation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Segmentation-free kidney localization and volume estimation using aggregated orthogonal decision CNNs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Amir-Khalili, Alborz and Hamarneh, Ghassan and Abugharbieh, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Medical Image Computing and Computer-Assisted Intervention}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{612--620}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI-MLMI</abbr></div> <div id="hussain2017collage" class="col-sm-8"> <div class="title">Collage CNN for renal cell carcinoma detection from CT</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Alborz Amir-Khalili, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Abugharbieh</a> </div> <div class="periodical"> <em>In International Workshop on Machine Learning in Medical Imaging</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-67389-9_27" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/Arafat_MLMI17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Renal cell carcinoma (RCC) is a common malignancy that accounts for a steadily increasing mortality rate worldwide. Widespread use of abdominal imaging in recent years, mainly CT and MRI, has significantly increased the detection rates of such cancers. However, detection still relies on a laborious manual process based on visual inspection of 2D image slices. In this paper, we propose an image collage based deep convolutional neural network (CNN) approach for automatic detection of pathological kidneys containing RCC. Our collage approach overcomes the absence of slice-wise training labels, enables slice-reshuffling based data augmentation, and offers favourable training time and performance compared to 3D CNNs. When validated on clinical CT datasets of 160 patients from the TCIA database, our method classified RCC cases vs. normal kidneys with 98% accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2017collage</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Collage CNN for renal cell carcinoma detection from CT}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Amir-Khalili, Alborz and Hamarneh, Ghassan and Abugharbieh, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Workshop on Machine Learning in Medical Imaging}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{229--237}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">UMB</abbr></div> <div id="hussain2017strain" class="col-sm-8"> <div class="title">Strain-initialized robust bone surface detection in 3-D ultrasound</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://mech.ubc.ca/antony-hodgson/" target="_blank" rel="noopener noreferrer">Antony J Hodgson</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Abugharbieh</a> </div> <div class="periodical"> <em>Ultrasound in medicine &amp; biology</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0301562916303696?casa_token=krY2KlUluY8AAAAA:5y7hfrUvrHjJkcsVWHnUtw5g3Wa3KWuLRwL3oSu8Ggn6YjiCxEd166yYSrWmEingXL1H09FN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/Arafat_UMB17.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Three-dimensional ultrasound has been increasingly considered as a safe radiation-free alternative to radiation-based fluoroscopic imaging for surgical guidance during computer-assisted orthopedic interventions, but because ultrasound images contain significant artifacts, it is challenging to automatically extract bone surfaces from these images. We propose an effective way to extract 3-D bone surfaces using a surface growing approach that is seeded from 2-D bone contours. The initial 2-D bone contours are estimated from a combination of ultrasound strain images and envelope power images. Novel features of the proposed method include: (i) improvement of a previously reported 2-D strain imaging-based bone segmentation method by incorporation of a depth-dependent cumulative power of the envelope into the elastographic data; (ii) incorporation of an echo decorrelation measure-based weight to fuse the strain and envelope maps; (iii) use of local statistics of the bone surface candidate points to detect the presence of any bone discontinuity; and (iv) an extension of our 2-D bone contour into a 3-D bone surface by use of an effective surface growing approach. Our new method produced average improvements in the mean absolute error of 18% and 23%, respectively, on 2-D and 3-D experimental phantom data, compared with those of two state-of-the-art bone segmentation methods. Validation on 2-D and 3-D clinical in vivo data also reveals, respectively, an average improvement in the mean absolute fitting error of 55% and an 18-fold improvement in the computation time.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2017strain</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Strain-initialized robust bone surface detection in 3-D ultrasound}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hodgson, Antony J and Abugharbieh, Rafeef}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Ultrasound in medicine \&amp; biology}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{43}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{648--661}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI-MLMI</abbr></div> <div id="hussain2016segmentation" class="col-sm-8"> <div class="title">Segmentation-free estimation of kidney volumes in CT with dual regression forests</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, <a href="https://www.sfu.ca/computing/people/faculty/ghassanhamarneh.html" target="_blank" rel="noopener noreferrer">Ghassan Hamarneh</a>, Timothy W O’Connell, Mohammed F Mohammed, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Abugharbieh</a> </div> <div class="periodical"> <em>In International Workshop on Machine Learning in Medical Imaging</em> 2016 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-47157-0_19" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/MLMI2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Accurate estimation of kidney volume is essential for clinical diagnoses and therapeutic decisions related to renal diseases. Existing kidney volume estimation methods rely on an intermediate segmentation step that is subject to various limitations. In this work, we propose a segmentation-free, supervised learning approach that addresses the challenges of accurate kidney volume estimation caused by extensive variations in kidney shape, size and orientation across subjects. We develop dual regression forests to simultaneously predict the kidney area per image slice, and kidney span per image volume. We validate our method on a dataset of 45 subjects with a total of 90 kidney samples. We obtained a volume estimation accuracy higher than existing segmentation-free (by 72 %) and segmentation-based methods (by 82 %). Compared to a single regression model, the dual regression reduced the false positive area-estimates and improved volume estimation accuracy by 41 %. We also found a mean deviation of under 10 % between our estimated kidney volumes and those obtained manually by expert radiologists.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2016segmentation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Segmentation-free estimation of kidney volumes in CT with dual regression forests}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hamarneh, Ghassan and O’Connell, Timothy W and Mohammed, Mohammed F and Abugharbieh, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Workshop on Machine Learning in Medical Imaging}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{156--163}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2015</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICEEE</abbr></div> <div id="hussain2015compressively" class="col-sm-8"> <div class="title">Compressively sensed ultrasound radio-frequency data reconstruction using the combined curvelets and wave atoms basis</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, and Riad Mashrub Shourov</div> <div class="periodical"> <em>In 2015 International Conference on Electrical &amp; Electronic Engineering (ICEEE)</em> 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/7428257?casa_token=XICEk4II6TIAAAAA:g1EyVRJBif0dyvwfNNe9xHFZZvjfLYcwee5A-FKCcQvPHnkxzFUwhTN5IcmZzRowekltrJs" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2015a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we propose a novel data reconstruction method for the compressively sensed ultrasound radio-frequency (RF) data using the combined curvelets- and wave atoms- (CCW) based orthonormal basis. Typically, the curvelets-based reconstruction better preserves the image features while the wave atoms-based reconstruction better preserves the oscillatory patterns of the typical ultrasound RF signals. We exploit the advantages from both the sparsifying bases via concatenating them where the RF reconstruction is done from the larger coefficients of the combined basis. We show that the CCW-based reconstruction method better recovers the RF oscillatory patterns as well as preserves the image features better than those of the curvelets- and wave atoms-based reconstruction methods alone. We find improvement with respect to the current methods of approximately 58% and 64% in terms of the normalized mean square error for the reconstructed synthetic phantom and in vivo RF data, respectively. We also show visual performance improvement in the B-mode images of approximately 33% and 44% in terms of the mean structural similarity for the synthetic phantom and in vivo data, respectively.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2015compressively</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Compressively sensed ultrasound radio-frequency data reconstruction using the combined curvelets and wave atoms basis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Shourov, Riad Mashrub}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 International Conference on Electrical \&amp; Electronic Engineering (ICEEE)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{209--212}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICCIT</abbr></div> <div id="hussain2015towards" class="col-sm-8"> <div class="title">Towards real-time 3D geometric nonlinear diffusion filter and its application to CT and MR imaging</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Riad Mashrub Shourov, and Shamima Nasrin Khan</div> <div class="periodical"> <em>In 2015 18th International Conference on Computer and Information Technology (ICCIT)</em> 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/7488115?casa_token=It1Pi7OtLEsAAAAA:F9bEARHP7syb_INZH1bJxrC_2RkyV4wyK1XJfNj4EvzUqFd9oVBAr8-9Nir8y-ad3ZusOa8" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2015b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We propose two near real-time nonlinear anisotropic diffusion filtering (NADF) methods for the 2D and 3D X-ray computed tomography (CT) and magnetic resonance (MR) image denoising. Typically, NADFs are preferred for the medical image denoising due to its edge preserving feature though they are computationally expensive. Recently, a computation-time efficient 2D NADF has been proposed which uses local pixel intensity-based geometric parameters for diffusion. But it has limitations resulting from (i) its assumption that the neighboring pixels are non-noisy while deciding on an interrogated pixel being noisy or not, and (ii) its confinement of working only on a 2D image. Motivated from this, we propose an improved 2D NADF method that uses additional neighboring pixels in an effective way to lower the noise impact on the estimated geometric parameters. We also extend our 2D method into 3D that considers all the three directions for information diffusion. The performance of the proposed methods is evaluated using a 3D synthetic phantom, and in vivo CT and MR data which demonstrates an average signal-to-noise-ratio-gain improvement of approximately 58% in 2D and 96% in 3D phantom data, and approximately 79% in 2D and 127% in 3D in vivo data, compared to the state-of-the-art method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2015towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards real-time 3D geometric nonlinear diffusion filter and its application to CT and MR imaging}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Shourov, Riad Mashrub and Khan, Shamima Nasrin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 18th International Conference on Computer and Information Technology (ICCIT)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{462--467}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CAOS</abbr></div> <div id="hussain2015automatic" class="col-sm-8"> <div class="title">Automatic Bone Segmentation in Ultrasound using Combined Ultrasound Strain Imaging and Envelope Signal Power</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Pierre Guy, <a href="https://mech.ubc.ca/antony-hodgson/" target="_blank" rel="noopener noreferrer">Antony J Hodgson</a>, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Abugharbieh</a> </div> <div class="periodical"> <em>In 2015 International Meeting on Computer Assisted Orthopaedic Surgery (CAOS)</em> 2015 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/CAOS2015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2015automatic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic Bone Segmentation in Ultrasound using Combined Ultrasound Strain Imaging and Envelope Signal Power}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Guy, Pierre and Hodgson, Antony J and Abugharbieh, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 International Meeting on Computer Assisted Orthopaedic Surgery (CAOS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{CAOS}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MASc Thesis</abbr></div> <div id="hussain2015robust" class="col-sm-8"> <div class="title">Robust Bone Detection in Ultrasound Using Combined Strain Imaging and Envelope Signal Power Detection</div> <div class="author"> <em>Mohammad Arafat Hussain</em> </div> <div class="periodical"> 2015 </div> <div class="links"> <a href="https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0166292" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://open.library.ubc.ca/media/download/pdf/24/1.0166292/3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </li> </ol> <h2 class="year">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Ultrasonics</abbr></div> <div id="hussain2014lesion" class="col-sm-8"> <div class="title">Lesion edge preserved direct average strain estimation for ultrasound elasticity imaging</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Farzana Alam, Sharmin Akhtar Rupa, Rayhana Awwal, Soo Yeol Lee, and <a href="http://khasan.buet.ac.bd/" target="_blank" rel="noopener noreferrer">Md Kamrul Hasan</a> </div> <div class="periodical"> <em>Ultrasonics</em> 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.sciencedirect.com/science/article/pii/S0041624X13001686?casa_token=BkTjesAnkx8AAAAA:o0nLzpt357pVNIzqqgZyjv9HcAVKDrWhjjYGXTz6DnkTeUrTA6JfvdNg3NqkNgNvhC-0Nm9u" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2014a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Elasticity imaging techniques with built-in or regularization-based smoothing feature for ensuring strain continuity are not intelligent enough to prevent distortion or lesion edge blurring while smoothing. This paper proposes a novel approach with built-in lesion edge preservation technique for high quality direct average strain imaging. An edge detection scheme, typically used in diffusion filtering is modified here for lesion edge detection. Based on the extracted edge information, lesion edges are preserved by modifying the strain determining cost function in the direct-average-strain-estimation (DASE) method. The proposed algorithm demonstrates approximately 3.42–4.25 dB improvement in terms of edge-mean-square-error (EMSE) than the other reported regularized or average strain estimation techniques in finite-element-modeling (FEM) simulation with almost no sacrifice in elastographic-signal-to-noise-ratio (SNRe) and elastographic-contrast-to-noise-ratio (CNRe) metrics. The efficacy of the proposed algorithm is also tested for the experimental phantom data and in vivo breast data. The results reveal that the proposed method can generate a high quality strain image delineating the lesion edge more clearly than the other reported strain estimation techniques that have been designed to ensure strain continuity. The computational cost, however, is little higher for the proposed method than the simpler DASE and considerably higher than that of the 2D analytic minimization (AM2D) method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2014lesion</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Lesion edge preserved direct average strain estimation for ultrasound elasticity imaging}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Alam, Farzana and Rupa, Sharmin Akhtar and Awwal, Rayhana and Lee, Soo Yeol and Hasan, Md Kamrul}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Ultrasonics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{54}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{137--146}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MICCAI</abbr></div> <div id="hussain2014robust" class="col-sm-8"> <div class="title">Robust bone detection in ultrasound using combined strain imaging and envelope signal power detection</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Antony Hodgson, and <a href="https://bisicl.ece.ubc.ca/rafeef/" target="_blank" rel="noopener noreferrer">Rafeef Abugharbieh</a> </div> <div class="periodical"> <em>In International conference on medical image computing and computer-assisted intervention</em> 2014 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/chapter/10.1007/978-3-319-10404-1_45" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/MICCAI2014.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Bone localization in ultrasound (US) remains challenging despite encouraging advances. Current methods, e.g. local image phase-based feature analysis, showed promising results but remain reliant on delicate parameter selection processes and prone to errors at confounding soft tissue interfaces of similar appearance to bone interfaces. We propose a different approach combining US strain imaging and envelope power detection at each radio-frequency (RF) sample. After initial estimation of strain and envelope power maps, we modify their dynamic ranges into a modified strain map (MSM) and a modified envelope map (MEM) that we subsequently fuse into a single combined map that we show corresponds robustly to actual bone boundaries. Our quantitative results demonstrate a marked reduction in false positive responses at soft tissue interfaces and an increase in bone delineation accuracy. Comparisons to the state-of-the-art on a finite-element-modelling (FEM) phantom and fiducial-based experimental phantom show an average improvement in mean absolute error (MAE) between actual and estimated bone boundaries of 32% and 14%, respectively. We also demonstrate an average reduction in false bone responses of 87% and 56%, respectively. Finally, we qualitatively validate on clinical in vivo data of the human radius and ulna bones, and demonstrate similar improvements to those observed on phantoms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hussain2014robust</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust bone detection in ultrasound using combined strain imaging and envelope signal power detection}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Hodgson, Antony and Abugharbieh, Rafeef}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International conference on medical image computing and computer-assisted intervention}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{356--363}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2014}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer, Cham}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TUFFC</abbr></div> <div id="hasan2013using" class="col-sm-8"> <div class="title">Using nearest neighbors for accurate estimation of ultrasonic attenuation in the spectral domain</div> <div class="author"> <a href="http://khasan.buet.ac.bd/" target="_blank" rel="noopener noreferrer">Md Kamrul Hasan</a>, <em>Mohammad Arafat Hussain</em>, Sharmin R Ara, Soo Yeol Lee, and S Kaisar Alam</div> <div class="periodical"> <em>IEEE transactions on ultrasonics, ferroelectrics, and frequency control</em> 2013 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/6521059?casa_token=2s5boH1hizUAAAAA:eI8qGb0LDefeIogW6kTDUZ0fV26Yp8l9Z9gOL-G4hJ8VrkrlDKwO4SfH1Fn1vA69zdiooG4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2013a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Attenuation is a key diagnostic parameter of tissue pathology change and thus may play a vital role in the quantitative discrimination of malignant and benign tumors in soft tissue. In this paper, two novel techniques are proposed for estimating the average ultrasonic attenuation in soft tissue using the spectral domain weighted nearest neighbor method. Because the attenuation coefficient of soft tissues can be considered to be a continuous function in a small neighborhood, we directly estimate an average value of it from the slope of the regression line fitted to the 1) modified average midband fit value and 2) the average center frequency shift along the depth. To calculate the average midband fit value, an average regression line computed from the exponentially weighted short-time Fourier transform (STFT) of the neighboring 1-D signal blocks, in the axial and lateral directions, is fitted over the usable bandwidth of the normalized power spectrum. The average center frequency downshift is computed from the maximization of a cost function defined from the normalized spectral cross-correlation (NSCC) of exponentially weighted nearest neighbors in both directions. Different from the large spatial signal-block-based spectral stability approach, a costfunction- based approach incorporating NSCC functions of neighboring 1-D signal blocks is introduced. This paves the way for using comparatively smaller spatial area along the lateral direction, a necessity for producing more realistic attenuation estimates for heterogeneous tissue. For accurate estimation of the attenuation coefficient, we also adopt a reference-phantombased diffraction-correction technique for both methods. The proposed attenuation estimation algorithm demonstrates better performance than other reported techniques in the tissue-mimicking phantom and the in vivo breast data analysis.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hasan2013using</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Using nearest neighbors for accurate estimation of ultrasonic attenuation in the spectral domain}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hasan, Md Kamrul and Hussain, Mohammad Arafat and Ara, Sharmin R and Lee, Soo Yeol and Alam, S Kaisar}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE transactions on ultrasonics, ferroelectrics, and frequency control}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{60}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1098--1114}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2013}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">MSc Thesis</abbr></div> <div id="hussain2013average" class="col-sm-8"> <div class="title">Average Strain Estimation for Ultrasound Elastography Using Exponentially Weighted Nearest Neighbors</div> <div class="author"> <em>Mohammad Arafat Hussain</em> </div> <div class="periodical"> 2013 </div> <div class="links"> <a href="http://lib.buet.ac.bd:8080/xmlui/handle/123456789/3451" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://app.box.com/s/xhqnrd7zzz6tkmauvav2sjtywwu2gbaz" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </li> </ol> <h2 class="year">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">IEEE TUFFC</abbr></div> <div id="hussain2012direct" class="col-sm-8"> <div class="title">Direct and gradient-based average strain estimation by using weighted nearest neighbor cross-correlation peaks</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Emran Mohammad Abu Anas, S Kaisar Alam, Soo Yeol Lee, and <a href="http://khasan.buet.ac.bd/" target="_blank" rel="noopener noreferrer">Md Kamrul Hasan</a> </div> <div class="periodical"> <em>IEEE transactions on ultrasonics, ferroelectrics, and frequency control</em> 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/6264135?casa_token=PIy45Ng4P0IAAAAA:oJcEziPV396ApM43mXRcCpUXmXhcqplWD-qOn4xAwO0OR3KPAr7V45m_YOU3z-AzQ82qyhg" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2012a.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, two novel approaches, gradientbased and direct strain estimation techniques, are proposed for high-quality average strain imaging incorporating a cost function maximization. Stiffness typically is a continuous function. Consequently, stiffness of proximal tissues is very close to that of the tissue corresponding to a given data window. Hence, a cost function is defined from exponentially weighted neighboring pre- and post-compression RF echo normalized cross-correlation peaks in the lateral (for displacement estimation) or in both the axial and the lateral (for direct strain estimation) directions. This enforces a controlled continuity in displacement/strain and average displacement/strain is calculated from the corresponding maximized cost function. Axial stress causes lateral shift in the tissue. Therefore, a 1-D post-compression echo segment is selected by incorporating Poisson’s ratio. Two stretching factors are considered simultaneously in gradient-based strain estimation that allow imaging the lesions properly. The proposed time-domain gradient-based and direct-strain-estimation-based algorithms demonstrate significantly better performance in terms of elastographic signal-to-noise ratio (SNRe), elastographic contrast-to-noise ratio (CNRe), peak signal-to-noise ratio (PSNR), and mean structural similarity (MSSIM) than the other reported time-domain gradientbased and direct-strain-estimation techniques in finite element modeling (FEM) simulation and phantom experiments. For example, in FEM simulation, it has been found that the proposed direct strain estimation method can improve up to approximately 2.49 to 8.71, 2.2 to 6.63, 1.5 to 5, and 1.59 to 2.45 dB in the SNRe, CNRe, PSNR, and MSSIM compared with the traditional direct strain estimation method, respectively, and the proposed gradient-based algorithm demonstrates 2.99 to 16.26, 18.74 to 23.88, 3 to 9.5, and 0.6 to 5.36 dB improvement in the SNRe, CNRe, PSNR, and MSSIM, respectively, compared with a recently reported time-domain gradient-based technique. The range of improvement as noted above is for low to high applied strains. In addition, the comparative results using the in vivo breast data (including malignant or benign masses) also show that the lesion size is better defined by the proposed gradient-based average strain estimation technique.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2012direct</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Direct and gradient-based average strain estimation by using weighted nearest neighbor cross-correlation peaks}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Anas, Emran Mohammad Abu and Alam, S Kaisar and Lee, Soo Yeol and Hasan, Md Kamrul}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE transactions on ultrasonics, ferroelectrics, and frequency control}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{59}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1713--1728}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Ultras. Imag.</abbr></div> <div id="hussain2012robust" class="col-sm-8"> <div class="title">Robust strain-estimation algorithm using combined radiofrequency and envelope cross-correlation with diffusion filtering</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, S Kaisar Alam, Soo Yeol Lee, and <a href="http://khasan.buet.ac.bd/" target="_blank" rel="noopener noreferrer">Md Kamrul Hasan</a> </div> <div class="periodical"> <em>Ultrasonic imaging</em> 2012 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://journals.sagepub.com/doi/abs/10.1177/016173461203400203?casa_token=BsoXF5galF8AAAAA:a6IOEWbjSXOOFT4xm6HjaWpQp411uUccdJsJLRy9pWZp7LPiJBpH1kxWMXYTztQTIroEIBM4rE0" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2012b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In ultrasound elastography, the strain in compressed tissue due to external deformation is estimated and is smaller in harder than softer tissue. With increased stress, the nonaxial motions of tissue elements increase and result in noisier strain images. At high strain, the envelope of the rf signal exhibits robustness to signal decorrelation. However, the precision of strain estimates using envelope signals is much worse compared to that using the rf signals. In this paper, we propose a novel approach for robust strain estimation by combining weighted rf cross-correlation and envelope cross-correlation functions. An applied strain-dependent piecewise-linear-weight is used for this purpose. In addition, we introduce nonlinear diffusion filtering to further enhance the resulting strain image. The results of our algorithm are demonstrated for up to 10% applied strain using a finite-element modelling (FEM) simulation phantom. It reveals that the elastographic signal-to-noise ratio (SNRe) and the elastographic contrast-to-noise ratio (CNRe) of the strain images can be improved more significantly than with other algorithms used in this paper. In addition, comparative results in terms of the mean structural similarity (MSSIM) using in vivo breast data show that the strain image quality can be improved noticeably by the proposed method than with the techniques employed in this work.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">hussain2012robust</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Robust strain-estimation algorithm using combined radiofrequency and envelope cross-correlation with diffusion filtering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hussain, Mohammad Arafat and Alam, S Kaisar and Lee, Soo Yeol and Hasan, Md Kamrul}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Ultrasonic imaging}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{93--109}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2012}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{SAGE Publications Sage CA: Los Angeles, CA}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ABS-AIUM</abbr></div> <div id="hussain2012improved" class="col-sm-8"> <div class="title">Improved Elasticity Imaging by Maximizing the Weighted Peaks of the Nearest Neighbor Cross-correlation Function</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, Emran Mohammad Abu Anas, S Kaisar Alam, Soo Yeol Lee, and <a href="http://khasan.buet.ac.bd/" target="_blank" rel="noopener noreferrer">Md Kamrul Hasan</a> </div> <div class="periodical"> <em>In 2012 American Institution of Ultrasound in Medicine (AIUM) Annual Convention and Preconvention Program</em> 2012 </div> <div class="links"> <a href="/assets/pdf/mah2012c.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ABS-UITS</abbr></div> <div id="hussain2012robusu" class="col-sm-8"> <div class="title">A Robust Strain Estimation Algorithm Using Combined Radio-frequency and Envelope Cross-correlation</div> <div class="author"> <em>Mohammad Arafat Hussain</em>, S Kaisar Alam, Soo Yeol Lee, and <a href="http://khasan.buet.ac.bd/" target="_blank" rel="noopener noreferrer">Md Kamrul Hasan</a> </div> <div class="periodical"> <em>In Ultrasonic Imaging and Tissue Characterization Symposium</em> 2012 </div> <div class="links"> <a href="/assets/pdf/mah2012d.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> </div> </div> </li> </ol> <h2 class="year">2011</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">BSc Thesis</abbr></div> <div id="hussain2011ultrasound" class="col-sm-8"> <div class="title">Ultrasound Strain Imaging in Wavelet Domain</div> <div class="author"> Md Tauhidul* Islam, and <em>Mohammad Arafat* Hussain</em> </div> <div class="periodical"> 2011 </div> <div class="links"> <a href="https://app.box.com/s/5ia82d9nu3wbscz9nmfyeys1rifx7bu3" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> </div> </div> </li></ol> <h2 class="year">2010</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">UKSim</abbr></div> <div id="kabir2010non" class="col-sm-8"> <div class="title">Non-linear down-sampling and signal reconstruction, without folding</div> <div class="author"> Hussain Mohammed Dipu Kabir, Syed Bahauddin Alam, Md Isme Azam, <em>Mohammad Arafat Hussain</em>, ABM Rafi Sazzad, Md Nazmus Sakib, and Md Abdul Matin</div> <div class="periodical"> <em>In 2010 Fourth UKSim European Symposium on Computer Modeling and Simulation</em> 2010 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/5703672?casa_token=88mmjYne1iMAAAAA:9yQ6oJc_GOmZWNwEJcjLgM4zjCOe6eXiflpaxh3KwwamTd1v804e8-ETEK5IzAWFxzUEGsc" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/mah2010.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents a theory of 1.5 factor nonlinear down-sampling, reconstruction and noise elimination. For linear down sampling of two or three factor, one sample is taken and next one or two samples are not taken/discarded. Here in non-linear down sampling two or three samples are taken and the next one is not taken. The purpose of this nonlinear down sampling is to send less data samples in voice communication. Though one sample is discarded after taking two samples value of this sample can be reconstructed from values of other samples. Here, two samples are at original sampling period, T s interval and next two samples are at 2T s interval. High-frequency sharp changes were extracted when sampled at T s interval. From received signal, discarded sample can be reconstructed from nearby four samples (Previous two and next two). When original signal contains higher frequency some error signal is introduced, after reconstruction. This error signal depends on original signal. Error signal is eliminated using original signal. Down-sampling is performed after sampling and signal reconstruction is performed just before hearing the sound.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">kabir2010non</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Non-linear down-sampling and signal reconstruction, without folding}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kabir, Hussain Mohammed Dipu and Alam, Syed Bahauddin and Azam, Md Isme and Hussain, Mohammad Arafat and Sazzad, ABM Rafi and Sakib, Md Nazmus and Matin, Md Abdul}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2010 Fourth UKSim European Symposium on Computer Modeling and Simulation}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{142--146}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2010}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Mohammad Arafat Hussain, PhD. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>